{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./wikidata_mapping_matched.json\",\"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "extracted_list = []\n",
    "for item in data[\"mapping_results\"]:   \n",
    "    extracted_info = {\n",
    "        \"wikidata_id\": item[\"wikidata_id\"],\n",
    "        \"wikidata_label\": item[\"wikidata_label\"],\n",
    "        \"data_types\": [sub_item[0] for sub_item in item.get(\"merged_types\", [])]\n",
    "    }\n",
    "    extracted_list.append(extracted_info)\n",
    "\n",
    "with open(\"./wikidata_mapping_extracted.json\", \"w\") as f:\n",
    "    json.dump(extracted_list, f, indent=4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique wikidata_id: 889\n"
     ]
    }
   ],
   "source": [
    "with open(\"./wikidata_mapping_extracted.json\",\"r\") as f:\n",
    "    data = json.load(f)\n",
    "unique_wikidata_ids = {item[\"wikidata_id\"] for item in data if \"wikidata_id\" in item}\n",
    "\n",
    "print(f\"Unique wikidata_id: {len(unique_wikidata_ids)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "889\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "with open(\"./wikidata_mapping_extracted.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "merged_data = defaultdict(lambda: {\"wikidata_label\": \"\", \"data_types\": set()})\n",
    "\n",
    "for item in data:\n",
    "    if \"wikidata_id\" in item and \"wikidata_label\" in item and \"data_types\" in item:\n",
    "        wid = item[\"wikidata_id\"]\n",
    "        merged_data[wid][\"wikidata_label\"] = item[\"wikidata_label\"]  \n",
    "        merged_data[wid][\"data_types\"].update(item[\"data_types\"]) \n",
    "\n",
    "merged_list = [\n",
    "    {\"wikidata_id\": wid, \"wikidata_label\": info[\"wikidata_label\"], \"data_types\": list(info[\"data_types\"])}\n",
    "    for wid, info in merged_data.items()\n",
    "]\n",
    "\n",
    "with open(\"./wikidata_mapping_merged.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(merged_list, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(len(merged_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 读取 JSON 文件\n",
    "with open(\"wikidata_mapping_extracted.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 解析 JSON，将 data_types 拆分成多行\n",
    "rows = []\n",
    "for entry in data:\n",
    "    wikidata_id = entry[\"wikidata_id\"]\n",
    "    wikidata_label = entry[\"wikidata_label\"]\n",
    "    data_types = entry[\"data_types\"]  # 可能是多个\n",
    "\n",
    "    for dtype in data_types:\n",
    "        rows.append([dtype, wikidata_label, wikidata_id])\n",
    "\n",
    "# 创建 DataFrame\n",
    "df = pd.DataFrame(rows, columns=[\"datatype\", \"wiki_word\", \"qid\"])\n",
    "\n",
    "# 保存为 CSV\n",
    "df.to_csv(\"datatype_wiki.csv\", index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "# 读取 JSON 文件\n",
    "with open(\"./wikidata_isolated_matched.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 解析 JSON，将 data_types 拆分成多行\n",
    "rows = []\n",
    "for entry in data[\"isolated_type\"]:\n",
    "    wikidata_id = entry[\"wikidata_id\"]\n",
    "    wikidata_label = entry[\"wikidata_label\"]\n",
    "    data_types = entry[\"datatype\"]  \n",
    "\n",
    "    rows.append([data_types, wikidata_label, wikidata_id])\n",
    "\n",
    "# 创建 DataFrame\n",
    "df = pd.DataFrame(rows, columns=[\"datatype\", \"wiki_word\", \"qid\"])\n",
    "\n",
    "print(len(df['qid'].dropna().unique().tolist()))\n",
    "\n",
    "# 保存为 CSV\n",
    "df.to_csv(\"datatype_isolated_wiki.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3894\n",
      "(3894, 3)\n",
      "(3894, 3)\n"
     ]
    }
   ],
   "source": [
    "# merge two csv files\n",
    "df1 = pd.read_csv(\"datatype_wiki.csv\")\n",
    "df2 = pd.read_csv(\"datatype_isolated_wiki.csv\")\n",
    "df = pd.concat([df1, df2])\n",
    "print(len(df['datatype'].dropna().unique().tolist()))\n",
    "df.to_csv(\"datatype_wiki_merged.csv\", index=False, encoding=\"utf-8\")\n",
    "print(df.shape)\n",
    "# delete the duplicated rows\n",
    "df = df.drop_duplicates()\n",
    "df.to_csv(\"datatype_wiki_merged.csv\", index=False, encoding=\"utf-8\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = pd.read_csv('./datatypes_relations_two_end.csv')\n",
    "\n",
    "# change the column name from 'word' to 'qid'\n",
    "data_types = data_types.rename(columns={'word': 'qid_1'})\n",
    "data_types = data_types.rename(columns={'word2': 'qid_2'})\n",
    "\n",
    "data_types = data_types.rename(columns={'word_label': 'wiki_word_1'})\n",
    "data_types = data_types.rename(columns={'word2_label': 'wiki_word_2'})\n",
    "\n",
    "data_types.to_csv('datatypes_relations_two_end.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "# 读取 CSV 文件\n",
    "datatype_wiki = pd.read_csv(\"./datatype_wiki_merged.csv\")\n",
    "relations = pd.read_csv(\"./datatypes_relations_two_end.csv\")\n",
    "\n",
    "# 处理 qid_to_datatype，使其支持一对多映射\n",
    "qid_to_datatype = datatype_wiki.groupby(\"qid\")[\"datatype\"].apply(list).to_dict()\n",
    "\n",
    "# 处理 qid_to_wiki_word（唯一映射）\n",
    "qid_to_wiki_word = dict(zip(datatype_wiki[\"qid\"], datatype_wiki[\"wiki_word\"]))\n",
    "\n",
    "# 展开 relations，每个 qid 可能对应多个 datatype，需要拆分成多行\n",
    "expanded_rows = []\n",
    "\n",
    "for _, row in relations.iterrows():\n",
    "    qid_1, qid_2, relation, relation_label = row[\"qid_1\"], row[\"qid_2\"], row[\"relation\"], row[\"relation_label\"]\n",
    "    \n",
    "    # 获取 qid_1 和 qid_2 对应的 datatypes（如果不存在，则为空列表）\n",
    "    datatypes_1 = qid_to_datatype.get(qid_1, [\"\"])  \n",
    "    datatypes_2 = qid_to_datatype.get(qid_2, [\"\"])  \n",
    "\n",
    "    # 获取 wiki_word\n",
    "    wiki_word_1 = qid_to_wiki_word.get(qid_1, \"\")\n",
    "    wiki_word_2 = qid_to_wiki_word.get(qid_2, \"\")\n",
    "\n",
    "    # 生成所有 (datatype_1, datatype_2) 组合，并展开成多行\n",
    "    for data_1, data_2 in product(datatypes_1, datatypes_2):\n",
    "        expanded_rows.append([qid_1, wiki_word_1, data_1, relation,relation_label, qid_2, wiki_word_2, data_2])\n",
    "\n",
    "# 创建新的 DataFrame\n",
    "expanded_df = pd.DataFrame(expanded_rows, columns=[\"qid_1\",\"wiki_word_1\", \"datatype_1\", \"relation\",\"relation_label\", \"qid_2\", \"wiki_word_2\",\"datatype_2\"])\n",
    "\n",
    "# 保存结果\n",
    "expanded_df.to_csv(\"expanded_datatype_relations_two_end.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
